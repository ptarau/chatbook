Department of Computer Science and Engineering .
We reorganize dependency graphs to focus on the most relevant content elements of a sentence , integrate sentence identifiers as graph nodes and after ranking the graph we take advantage of the implicit semantic information that dependency links bring in the form of subject-verb-object , `` is-a '' and `` part-of '' relations .
Working on the Prolog facts and their inferred consequences , the dialog engine specializes the text graph with respect to a query and reveals interactively the document 's most relevant content elements .
Introduction Logic programming languages have been used successfully for inference and planning tasks on restricted domain natural language processing tasks but not much on open domain , large scale information retrieval and knowledge representation tasks .
On the other hand , deep learning systems are very good at basic tasks ranging from parsing to factoid-trained question answering systems , but still taking baby steps when emulating human-level inference processing on complex documents .
Thus , a significant gap persists between neural and symbolic processing in the field .
We will start with a quick overview of the tools and techniques needed .
In our case , this means integrating a declarative language module , focusing on high level text mining , into the Python-based nltk ecosystem , while relying on the Java-based Stanford CoreNLP toolkit for basic tasks like segmentation , part-of-speech tagging and parsing .
Overview of the System Architecture Fig .
summarizes the architecture of our system .
The Stanford parser is started as a separate server process to which the Python text processing module connects as a client .
It interfaces with the Prolog-based dialog engine by generating a clausal representation of the document 's structure and content , as well as the user 's queries .
The dialog engine is responsible for handling the user 's queries for which answers are sent back to the Python front-end which handles also the call to OS-level spoken-language services , when activated .
System Architecture Today 's dependency parsers , among which the neurally-trained Stanford dependency parser stands out , produce highly accurate dependency graphs and part of speech tagged vertices .
Inspired by the effectiveness of algorithms like Google 's PageRank , recursive ranking algorithms applied to text graphs have enabled extraction of keyphrases , summaries and relations .
Their popularity continues to increase due to the holistic view they shed on the interconnections between text units that act as recommenders for the most relevant ones , as well as the comparative simplicity of the algorithms .
At close to 3000 citations and a follow-up of some almost equally as highly cited papers like the TextRank algorithm and its creative descendants have extended their applications to a wide variety of document types and social media interactions in a few dozen languages .
While part of the family of the TextRank descendants , our graph based text processing algorithm will use information derived from the dependency graphs associated to sentences .
With help from the labels marking the edges of a dependency graph and the part of speech tags associated to its nodes , we will extract rank-ordered facts corresponding to content elements present in sentences .
We pass these to logic programs that can query them and infer new relations , beyond those that can be mined directly from the text .
Like in the case of a good search engine , interaction with a text document will focus on the most relevant and semantically coherent elements matching a query .
With this in mind , the natural feel of an answer syntactically appropriate for a query is less important than the usefulness of the content elements extracted : just sentences of the document , in their natural order .
We will also enable spoken interaction with the dialog engine , opening doors for the use of the system via voice-based appliances .
Applications range from assistive technologies to visually challenged people , live user manuals , teaching from K-12 to graduate level and interactive information retrieval from complex technical or legal documents .
The most significant contributions of the research work covered by the paper are : a logic relation post-processor supporting realtime interactive queries about a document 's content integration of our algorithms into an open-source system with practical uses helping a reader of a scientific document to interactively familiarize herself with its content The paper is organized as follows .
Section puts in context the main ideas of the paper and justifies some of the architecture choices we have made .
Section overviews related work and background information .
We start with the building and the ranking of the text graph .
Then , we overview the summary , keyphrase and relation extraction and the creation of the Prolog database that constitutes the logical model of the document , to be processed by the dialog engine .
Building and ranking the text graph We connect as a Python client to the Stanford CoreNLP server and use it to provide our dependency links via the wrapper at https : //www.nltk.org/ of the Stanford CoreNLP toolkit .
Unlike the original TextRank and related approaches that develop special techniques for each text processing task , we design a unified algorithm to obtain graph representations of documents , that are suitable for keyphrase extraction , summarization and interactive content exploration .
as nodes of the text graph .
As keyphrases are centered around nouns and good summary sentences are likely to talk about important concepts , we will need to reverse some links in the dependency graph provided by the parser , to prioritize nouns and deprioritize verbs , especially auxiliary and modal ones .
Thus , we redirect the dependency edges toward nouns with subject and object roles , as shown for a simple short sentence in Fig .
depgraphDependency graph of a simple sentence with redirected and newly added arrows 0.24depgraph.pdf We also create `` recommend '' links from words to the sentence identifiers and back from sentences to verbs with predicate roles to indirectly ensure that sentences recommend and are recommended by their content .
Specifically , we ensure that sentences recommend verbs with predicate function from where their recommendation spreads to nouns relevant as predicate arguments ( e.g. , having subject or object roles ) .
By using the PageRank implementation of the networkx toolkit ( https : //networkx.github.io/ ) , after ranking the sentence and word nodes of the text graph , the system is also able to display subgraphs filtered to contain only the highest ranked nodes , using Python 's graphviz library .
constitText graph of the highest ranked words in the U.S. Constitution0.60constit Pre- and post-ranking graph refinements The algorithm induces a form of automatic stopword filtering , due to the fact that our dependency link arrangement ensures that modifiers with lesser semantic value relinquish their rank by pointing to more significant lexical components .
This is a valid alternative to explicit `` leaf trimming '' before ranking , which remains an option for reducing graph size for large texts or multi-document collections as well as helping with a more focussed relation extraction from the reduced graphs .
Besides word-to-word links , our text graphs connect sentences as additional dependency graph nodes , resulting in a unified keyphrase and summary extraction framework .
Note also that , as an option that is relevant especially for scientific , medical or legal documents , we add firstin links from a word to the sentence containing its first occurrence , to prioritize sentences where concepts are likely to be defined or explained .
Our reliance on graphs provided by dependency parsers builds a bridge between deep neural network-based machine learning and graph-based natural language processing enabling us to often capture implicit semantic information .
Summary and keyword extraction As link configurations tend to favor very long sentences , a post-ranking normalization is applied for sentence ranking .
After ordering sentences by rank we extract the highest ranked ones and reorder them in their natural order in the text to form a more coherent summary .
We use the parser 's compound phrase tags to fuse along dependency links .
We design our keyphrase synthesis algorithm to ensure that highly ranked words will pull out their contexts from sentences , to make up meaningful keyphrases .
As a heuristic , we mine for a context of 2-4 dependency linked words of a highly ranked noun , while ensuring that the context itself has a high-enough rank , as we compute a weighted average favoring the noun over the elements of its context .
We plan in the future to also generate relations from conditional statements identified following dependency links and involving negations , modalities , conjuncts and disjuncts , to be represented as Prolog rules .
Subject-verb-object ( SVO ) relations are extracted directly from the dependency graph and an extra argument is added to the triplet marking the number of the sentence they originate from .
As a heuristic that ensures that they are relevant to the content of the text , we ensure that both their arguments are words that occur in the document , when connecting their corresponding synsets via WordNet relations .
The Prolog-based dialog engine After our Python-based document processor , with help from the Stanford dependency parser , builds and ranks the text graph and extracts summaries , keyphrases and relations , we pass them to the Prolog-based dialog engine .
Generating input for post-processing by logic programs Once the document is processed , we generate , besides the dependency links provided by the parser , relations containing facts that we have gleaned from processing the document .
To keep the interface simple and portable to other logic programming tools , we generate the following predicates in the form of Prolog-readable code , in one file per document : keyword ( WordPhrase ) .
- edge marked with sentence identifiers indicating where it was extracted from , and the lemmas with their POS tags at the two ends of the edge rank ( LemmaOrSentenceId , Rank ) .
- the list of sentences in the document with a sentence identifier as first argument and a list of words as second argument They provide a relational view of a document in the form of a database that will support the inference mechanisms built on top of it .
The resulting logic program can then be processed with Prolog semantics , possibly enhanced by using constraint solvers , abductive reasoners or via Answer Set Programming systems .
Specifically , we expect benefits from such extensions for tackling computationally difficult problems like word-sense disambiguation ( WSD ) or entailment inference as well as domain-specific reasoning .
We have applied this process to the Krapivin document set , a collection of 2304 research papers annotated with the authors ' own keyphrases and abstracts .
After the adaptor creates the Prolog process and the content of the digested document is transferred from Python ( in a few seconds for typical scientific paper sizes of 10-15 pages ) , query processing is realtime .
The user interaction loop With the Prolog representation of the digested document in memory , the dialog starts by displaying the summary and keyphrases extracted from the document ( And also speak them out if the quiet flag is off . ) .
The dialog agent associated to the document answers queries as sets of salient sentences extracted from the text , via a specialization of our summarization algorithm to the context inferred from the query .
As part of an interactive read/listen , evaluate , print/say loop , we generate for each query sentence , a set of predicates that are passed to the Prolog process , from where answers will come back via the pyswip interface .
The predicates extracted from a query have the same structure as the database representing the content of the complete document , initially sent to Prolog .
The answer generation algorithm Answers are generated by selecting the most relevant sentences , presented in their natural order in the text , in the form of a specialized `` mini-summary '' .
Query expansion Answer generation starts with a query-expansion mechanism via relations that are derived by finding , for lemmas in the query , WordNet hypernyms , hyponyms , meronyms and holonyms , as well as by directly extracting them from the query 's dependency links .
We use the rankings available both in the query and the document graph to prioritize the highest ranked sentences connected to the highest ranked nodes in the query .
Short-time dialog memory We keep representations of recent queries in memory , as well as the answers generated for them .
If the representation of the current query overlaps with a past one , we use content in the past query 's database to extend query expansion to cover edges originating from that query .
Overlapping is detected via shared edges between noun or verb nodes between the query graphs .
Sentence selection Answer sentence selection happens by a combination of several interoperating algorithms : use of personalized PageRank with a dictionary provided by highest ranking lemmas and their ranks in the query 's graph , followed by reranking the document 's graph to specialize to the query 's content matching guided by SVO-relations matching of edges in the query graph against edges in the document graph query expansion guided by rankings in both the query graph and the document graph matching guided by a selection of related content components in the short-term dialog memory window Matching against the Prolog database representing the document is currently implemented as a size constraint on the intersection of the expanded query lemma set , built with highly ranked shared lemmas pointing to sentences containing them .
The set of answers is organized to return the highest-ranked sentences based on relevance to the query and in the order in which they appear in the document .
We keep the dialog window relatively small ( limited to the highest ranked 3 sentences in the answer set , by default ) .
Relevance is ensured with help from the rankings computed for both the document content and the query .
Interacting with the dialog engine The following example shows the result of a query on the US Constitution document .
59 : In Case of the Removal of the President from Office , or of his Death , Resignation , or Inability to discharge the Powers and Duties of the said Office , the same shall devolve on the Vice President , and the Congress may by Law provide for the Case of Removal , Death , Resignation or Inability , both of the President and Vice President , declaring what Officer shall then act as President , and such Officer shall act accordingly , until the Disability be removed , or a President shall be elected .
190 : If the Congress , within twenty one days after receipt of the latter written declaration , or , if Congress is not in session , within twenty one days after Congress is required to assemble , determines by two thirds vote of both Houses that the President is unable to discharge the powers and duties of his office , the Vice President shall continue to discharge the same as Acting President ; otherwise , the President shall resume the powers and duties of his office .
The dependency graph of the query is shown in Fig .
611 : In the example of the transmission of light just dealt with , we have seen that the general theory of relativity enables us to derive theoretically the influence of a gravitational field on the course of natural processes , the laws of which are already known when a gravitational field is absent .
764 : On the contrary , we arrived at the result that according to this latter theory the velocity of light must always depend on the co-ordinates when a gravitational field is present .
765 : In connection with a specific illustration in Section XXIII , we found that the presence of a gravitational field invalidates the definition of the coordinates and the time , which led us to our objective in the special theory of relativity .
lightGraph of query on Einstein 's book on Relativity0.30light The query graph is shown in Fig .
After the less than 30 seconds that it takes to digest the book , answers are generated in less than a second for all queries that we have tried .
Given the availability of spoken dialog , a user can iterate and refine queries to extract the most relevant answer sentences of a document .
3291 : Note : If a tire has been replaced or repaired using a different tire sealant than the one available from Tesla , and a low tire pressure is detected , it is possible that the tire sensor has been damaged .
The highly relevant first answer is genuinely useful in this case , given that Tesla Model 3 's do not have a spare tire .
Being able to use voice queries while driving and in need of urgent technical information about one 's car , hints towards obvious practical applications of our dialog engine .
Discussion Ideally , one would like to evaluate the quality of natural language understanding of an AI system by querying it not only about a set of relations explicitly extracted in the text , but also about relations inferred from the text .
Moreover , one would like also to have the system justify the inferred relations in the form of a proof , or at least a sketch of the thought process a human would use for the same purpose .
On the other hand , simple relations , stated or implied by text elements that can be mined or inferred from a ranked graph built from labeled dependency links , provide a limited but manageable approximation of the text 's deeper logic structure , especially when aggregated with generalizations and similarities provided by WordNet or the much richer Wikipedia knowledge graph .
Given its effectiveness as an interactive content exploration tool , we plan future work on packaging our dialog engine as a set of Amazon Alexa skills for some popular Wikipedia entries as well as product reviews , FAQs and user manuals .
Empirical evaluation of our keyphrase and summarization algorithms will be subject to a different paper , but preliminary tests indicate that both of them match or exceed Rouge scores for state of the art systems .
Related workDependency parsing The Stanford neural network based dependency parser is now part of the Stanford CoreNLP toolkit ( https : //stanfordnlp.github.io/CoreNLP/ ) , which also comes with part of speech tagging , named entities recognition and co-reference resolution .
Its evolution toward the use of Universal Dependencies makes tools relying on it potentially portable to over 70 languages covered by the Universal Dependencies effort ( https : //universaldependencies.org/ ) .
Of particular interest is the connection of dependency graphs to logic elements like predicate argument relations .
The mechanism of automatic conversion of constituency trees to dependency graphs described in provides a bridge allowing the output of high-quality statistically trained phrase structure parsers to be reused for extraction of dependency links .
We analyze dependency links and POS-tags associated to their endpoints to extract SVO relations .
By redirecting links to focus on nouns and sentences we not only enable keyphrase and summary extraction from the resulting document graph but also facilitate its use for query answering in our dialog engine .
Graph based Natural Language Processing In TextRank keyphrases are using a co-occurrence relation , controlled by the distance between word occurrences : two vertices are connected if their corresponding lexical units co-occur within a sliding window of 2 to 10 words .
Sentence similarity is computed as content overlap giving weights on the links that refine the original PageRank algorithm .
TextRank needs elimination of stop words and reports best results when links are restricted to nouns and adjectives .
In several graph centrality measures are explored and offers a comprehensive overview on graph-based natural language processing and related graph algorithms .
Graph-based and other text summarization techniques are surveyed in and more recently in .
Besides ranking , elements like coherence via similarity with previously chosen sentences and avoidance of redundant rephrasings are shown to contribute to the overall quality of the summaries .
Beyond summaries obtained by aggregating important sentences extracted from a document , and possibly applying to them sentence compression techniques that remove redundant or less relevant words , new techniques are emerging for abstractive summarization .
For this purpose , in the context of graph-based processing , one clearly benefits from as much syntactic and semantic information as possible , given also the need to synthesize new sentences subject to syntactic and semantic constraints .
The main novelty of our approach in this context is building the text graph from dependency links and integrating words and sentences in the same text graph , resulting in a unified algorithm that also enables relation extraction and interactive text mining .
Relation Extraction The relevance of dependency graphs for relation extraction has been identified in several papers , with pointing out to their role as a generic interface between parsers and relation extraction systems .
Of particular interest for relation extraction facilitated by dependency graphs is the shortest path hypothesis that prefers relating entities like predicate arguments that are connected via a shortest paths in the graph .
The use of ranking algorithms in combination with WordNet synset links for word-sense disambiguation goes back as far as , in fact a prequel to the TextRank paper .
With the emergence of resources like Wikipedia , a much richer set of links and content elements has been used in connection with graph based natural language processing .
We currently extract our relations directly from the dependency graph and by using one step up and one step down links in the WordNet hypernym and meronym hierarchies , but extensions are planned to integrate Wikipedia content , via the dbpedia database ( https : //wiki.dbpedia.org/ ) and to extract more elaborate logic relations using a Prolog-based semantic parser like Boxer .
Logic Programming Systems for Natural Language Processing A common characteristic of Prolog or ASP-based NLP systems is their focus on closed domains with domain-specific logic expressed in clausal form , although recent work like extracts action language programs from more general narratives .
As our main objective is the building of a practically useful dialog agent , and as we work with open domain text and query driven content retrieval , our focus is not on precise domain-specific reasoning mechanisms .
By taking advantage of the Prolog representation of a document 's content , we use reasoning about the extracted relations and ranking information to find the most relevant sentences derived from a given query and the recent dialog history .
Conclusions The key idea of the paper has evolved from our search for synergies between symbolic AI and emerging machine-learning based natural language processing tools .
It is our belief that these are complementary and that by working together they will take significant forward steps in natural language understanding .
We have based our text graph on heterogeneous , but syntactically and semantically meaningful text units ( words and sentences ) resulting in a web of interleaved links , mutually recommending each other 's highly ranked instances .
Our fact extraction algorithm , in combination with the Prolog interface has elevated the syntactic information provided by dependency graphs with semantic elements ready to benefit from logic-based inference mechanisms .
Given the standardization brought by the use of Universal Dependencies , our techniques are likely to be portable to a large number of languages .
The Prolog-based dialog engine supports spoken interaction with a conversational agent that exposes salient content of the document driven by the user 's interest .
Its applications range from assistive technologies to visually challenged people , voice interaction with user manuals , teaching from K-12 to graduate level and interactive information retrieval from complex technical or legal documents .
Last but not least , we have used our system 's front end to generate the Prolog dataset at http : //www.cse.unt.edu/ tarau/datasets/PrologDeepRankDataset.zip , derived from more than 2000 research papers and made it available to other researchers using logic programming based reasoners and content mining tools .
Acknowledgment We are thankful to the anonymous reviewers of PADL'2020 for their careful reading and constructive suggestions .